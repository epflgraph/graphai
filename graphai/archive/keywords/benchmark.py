import pandas as pd
import yake
from keybert import KeyBERT

from graphai.core.text.keywords import rake_extract

from graphai.core.utils.breadcrumb import Breadcrumb

pd.set_option('display.max_rows', 400)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

text = """linearly repetitive"""
text = """Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud Next conference in San Francisco this week, the official announcement could come as early as tomorrow. Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. Google itself declined 'to comment on rumors'. Kaggle, which has about half a million data scientists on its platform, was founded by Goldbloom  and Ben Hamner in 2010. The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its specific niche. The service is basically the de facto home for running data science and machine learning competitions. With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow and other projects). Kaggle has a bit of a history with Google, too, but that's pretty recent. Earlier this month, Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google will keep the service running - likely under its current name. While the acquisition is probably more about Kaggle's community than technology, Kaggle did build some interesting tools for hosting its competition and 'kernels', too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can share this code on the platform (the company previously called them 'scripts'). Like similar competition-centric sites, Kaggle also runs a job board, too. It's unclear what Google will do with that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it's $12.75) since its   launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, Naval Ravikant, Google chief economist Hal Varian, Khosla Ventures and Yuri Milner"""
text = """I am doing some preliminary research on the potential of deepfakes to interfere with elections in Switzerland this year, following what was seen in Turkey during its election and the attack ads from Ron DeSantis."""
text = """We are interested in various aspects of spectral rigidity of Cayley and Schreier graphs of finitely generated groups. For each pair of integers, we consider an uncountable family of groups of automorphisms of the rooted d-regular tree, which provide examples of the following interesting phenomena. We get an uncountable family of non-quasi-isometric Cayley graphs with the same Laplacian spectrum, a union of two intervals, which we compute explicitly. Some of the groups provide examples where the spectrum of the Cayley graph is connected for one generating set and has a gap for another. We exhibit infinite Schreier graphs of these groups with the spectrum a Cantor set of Lebesgue measure zero union a countable set of isolated points accumulating on it. The Kesten spectral measures of the Laplacian on these Schreier graphs are discrete and concentrated on the isolated points. We construct, moreover, a complete system of eigenfunctions that are strongly localized."""
text = """The Fourth Industrial Revolution has considerably sped up the pace of skill changes in many professional domains, with scores of new skills emerging and many old skills moving towards obsolescence. For these domains, identifying the new necessary skills in a timely manner is a difficult task, where existing methods are inadequate. Understanding the process, by which these new skills and technologies appear in and diffuse through a professional domain, could give training providers more time to identify these new skills and react. For this purpose, in the present work, we look at the dynamics between online learning platforms and online hiring platforms in the software programming profession, a rapidly evolving domain. To do so, we fuse four data sources together: Stack Overflow, an online community questions and answers (Q&A) platform; Google Trends, which provides online search trends from Google; Udemy, a platform offering skill-based Massively Open Online Courses (MOOCs) where anyone can create courses; and Stack Overflow Jobs, a job ad platform. We place these platforms along two axes: i) how much expertise it takes, on average, to create content on them, and ii) whether, in general, the decision to create content on them is made by individuals or by groups. Our results show that the topics under study have a systematic tendency to appear earlier on platforms where content creation requires (on average) less expertise and is done more individually, rather than by groups: Stack Overflow is found to be more agile than Udemy, which is itself more agile than Stack Overflow Jobs (Google Trends did not prove usable due to extreme data sparsity). However, our results also show that this tendency is not present for all new skills, and that the software programming profession as a whole is remarkably agile: there are usually only a few months between the first Stack Overflow appearance of a new topic, and its first appearance on Udemy or Stack Overflow Jobs.    In addition, we find that Udemy’s agility has dramatically increased over time. Our novel methodology is able to provide valuable insights into the dynamics between online education and job ad platforms, enabling training program creators to look at said dynamics for various topics and to understand the pace of change. This allows them to maintain better awareness of the trends and to prioritize their attention, both on the right topics and on the right platforms."""
text = """A future e+/e− linear collider is planned to be built in order to obtain precise measurements (at the T eV scale) which would be complementary to the ones from the LHC. One of the challenges of this linear collider will be to focus the beam down to nanometer level transverse sizes at the interaction point, to obtain a high luminosity of a few 10−34cm2s−1. The two linear collider projects (ILC and CLIC) require beam delivery systems with the same local chromaticity correction scheme in the final focus. ATF2 at KEK (Japan), an implementation of this scheme scaled down in energy, uses the beam extracted from ATF, which is one of the most successful damping rings in the world. The ATF2 goals are to prove the feasibility and the stability of the linear collider final focus system and to define and test the experimental correction procedures. The nominal beam sizes at the interaction point are 3μm horizontally and 37nm vertically. The work in this thesis was started before the commissioning and covered its first year at KEK. At the beginning, we observed that the striplines BPMs were not working properly so we investigated their behavior in detail. The problem was characterized and later solved in 2010 by upgrading the electronics. We then developed an efficient procedure to check the modeling of the beam line, comparing measurements of transfer matrices to model predictions calculated on line. After obtaining a good agreement, we were able to successfully test the trajectory correction algorithm which had been developed, reducing the difference between BPM measurements and the target values down to 0.5mm horizontally and 0.2mm vertically. We also successfully developed an algorithm to reconstruct pulse to pulse beam trajectory fluctuations with sub-micron precision. This reconstruction also gave a precise determination of the energy fluctuation, allowing a global fit of the dispersion function along the beam line with a precision of a few mm, dominated by systematic errors from transfer matrices and BPMs scale factor uncertainties. A simple and robust IP beam size tuning method using sextupoles displacements was also studied in simulation, whose performance indicates that, given some assumptions on the error level of the beam, convergence within 20% of the nominal beam size should be possible in 8 hours with a 80% probability. The first experimental tests of such beam size tuning methods, based on measurements at the interaction point are on-going in 2010 and 2011."""
text = """Consider a nonparametric representation of acoustic wave fields that consists of observing the sound pressure along a straight line or a smooth contour L defined in space. The observed data contains implicit information of the surrounding acoustic scene, both in terms of spatial arrangement of the sources and their respective temporal evolution. We show that such data can be effectively analyzed and processed in what we call the space-time-frequency representation space, consisting of a Gabor representation across the spatio-temporal manifold defined by the spatial axis L and the temporal axis t. In the presence of a source, the spectral patterns generated at L have a characteristic triangular shape that changes according to certain parameters, such as the source distance and direction, the number of sources, the concavity of L, and the analysis window size. Yet, in general, the wave fronts can be expressed as a function of elementary directional components-most notably, plane waves and far-field components. Furthermore, we address the problem of processing the wave field in discrete space and time, i.e., sampled along L and t, where a Gabor representation implies that the wave fronts are processed in a block-wise fashion. The key challenge is how to chose and customize a spatio-temporal filter bank such that it exploits the physical properties of the wave field while satisfying strict requirements such as perfect reconstruction, critical sampling, and computational efficiency. We discuss the architecture of such filter banks, and demonstrate their applicability in the context of real applications, such as spatial filtering, deconvolution, and wave field coding."""

top_n = max(5, min(50, len(text) // 100))

bc = Breadcrumb()

yake_extractor = yake.KeywordExtractor(top=top_n)
keybert_extractor = KeyBERT()

bc.log('RAKE')
rake_keywords = rake_extract(text, use_nltk=False, return_scores=True)

bc.log('NLTK')
nltk_keywords = rake_extract(text, use_nltk=True, return_scores=True)
nltk_keywords = list(dict(nltk_keywords).items())

bc.log('YAKE')
yake_keywords = yake_extractor.extract_keywords(text)

bc.log('KeyBERT')
keybert_keywords = keybert_extractor.extract_keywords(text, keyphrase_ngram_range=(1, 3), top_n=top_n)

bc.log('YAKEBERT')
candidates = [str.lower(kwd) for kwd, score in yake_keywords]
yakebert_keywords = keybert_extractor.extract_keywords(text, candidates=candidates, keyphrase_ngram_range=(1, 3), top_n=top_n)

bc.report()

df = pd.concat([
    pd.DataFrame(rake_keywords, columns=['RAKE_kwd', 'RAKE_score']),
    pd.DataFrame(nltk_keywords, columns=['NLTK_kwd', 'NLTK_score']),
    pd.DataFrame(yake_keywords, columns=['YAKE_kwd', 'YAKE_score']),
    pd.DataFrame(keybert_keywords, columns=['KeyBERT_kwd', 'KeyBERT_score']),
    pd.DataFrame(yakebert_keywords, columns=['YAKEBERT_kwd', 'YAKEBERT_score'])
], axis=1)
print(df)
