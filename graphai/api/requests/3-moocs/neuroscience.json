{
	"raw_text": "So, in this section, what we are going to cover is an introduction to simulation neuroscience. First we are going to look at the approaches to study the brain and then we are going to go through the rationale, the principles, data strategy, informatics strategies, reconstruction strategies, and simulation strategies. So, after this course, this lecture, the first lecture, you should have an understanding of what simulation neuroscience is, you should have a general idea on how we treat data and our approach to informatics, what we mean by reconstruction and what kind of strategies we use to do reconstruction, tissue reconstruction, and what strategies we use for simulation. These are very general concepts. We are going to go in the next lectures much deeper and you should also have an understanding of what the caveats are for simulation neuroscience. So, we are going to look at the approaches to study the brain, because I think we should take a step back and say: how do we advance scientific knowledge? With our three branches of exploratory science if you wish to discover new knowledge. One of them is, of course experimental. That is the ground truth approach, you observe, you hypothesize, you test, and you make more measurements. For the brain of course, we can look at normal brains. Everything from genes, protein, cell, circuits, cognition, psychophysics, all the way up to behavior, and then of course you have nature's experiments which are the disease brain. You can look at the effect of toxins, effect of drugs, lesions on the brain, mutations that are occurring and degeneration with aging, and all of these provides you sort of a repertoire of conditions where you can actually try to understand what the brain is doing. The second is theoretical neuroscience or theory where you essentially hypothesize about the data or about the observations and one can look at them in at least four different categories. They are sort of informatics which really tries to look at the trends or correlations and patterns in the data. Theoretical neuroscience is trying to explain the data. You see the data, you want to predict outcomes of an experiment, and you have a theory about the data and your observation. Computational neuroscience is more about building a model to replicate the experimental data and generally, it is to build a minimal model and the idea is that if you can explain a phenomenon in the simplest possible way, then you have the deepest understanding. It is debatable, but that is the idea of computational neuroscience. Then you have applied neuroscience, where basically you can say that if I really do understand how neurons work, then I can build a device that will listen to them and I can develop a treatment that will cure them and that is applied neuroscience. Then there is simulation neuroscience and in neuroscience, this is relatively new. We're sort of driving this or trying to pioneer this. And the way to think of simulation neuroscience is that it unifies both experimental and theoretical approaches. The difference in simulation neuroscience is that you consider every detail of the brain. Nothing is left out. So for example, if you had a theory of how language works and you had a couple of equations that could build a machine that would be able to speak. That would be wonderful. It would indicate that you have a deep understanding of language, but it won't help you understand how a genetic mutation impacts language and how that affects the next neuron and the next circuit and the brain activity and how you should treat it. So simulation neuroscience is about taking all the data into account. All the knowledge into account and integrating it. It is also about filling the gaps of knowledge with hypotheses, because we may not be able to measure everything. Okay. So, another way to look at, to get a perspective on the way we approach the brain is that there are individual researchers and in the world there are probably about 200,000 individual neuroscientists that are exploring the brain. So they are looking at a genetic level, at the protein level, at the behavioral level and there is a huge repertoire of different techniques that can be used to be able to probe the brain and to assess what is going on. And this is producing a massive range of data and today there are about a 100,000 scientific papers produced by these individual research from all around the world. So there is an enormous amount of data being produced to understand the brain and we actually do know lots and lots of little pieces. In individual research, every lab has its own method, its own conditions, its own instruments, its own biases and of course, that means that it is not always easy to compare data and findings across laboratories. And it is quite a significant problem that in neuroscience, a significant amount of the research that has been done in individual and individual laboratories... ...are difficult to reproduce or impossible to reproduce, but nevertheless, they provide a massive source of data and knowledge. The second approach to the brain is to go large-scale, to go industrial scale. For example, the way that the Allen Institute approaches brain mapping or the Human Connectome Project or some parts of the Human Brain Project, the European Human Brain Project and the idea there is to standardize. In the case of the Allen Institute, for example, they would standardize all the methods and they will produce 20,000 atlases where each atlas is telling you where one gene is in the whole brain and so you have 20,000 genes, 20,000 atlases and they do that very systematically. So, you have very solid, large block of data. So this is large-scale brain mapping and it is really sort of industrial scale neuroscience. It is extremely valuable data for simulation neuroscience, because it is highly standardized and it is much easier to use. But the individual research is very valuable to actually validate the model, the reconstructed tissues that are built afterwards. So all data is valuable, all approaches are valuable. Then of course, you have to database the brain and this has a very long history. There is a lot of efforts that have gone into databasing the brain. It was started in 1990 by Bill Clinton actually. He started the US Human Brain Project, which was an effort to database the brain. It has been a very difficult, very long journey, because there are so many different types of data and different conditions and different questions. It is very difficult to decide how to database the brain, but there are now international efforts and it is also an effort in the European Human Brain Project to database the brain in a systematic way. It is not a solved problem. There are a lot of challenges and you are going to have a lecture about it in this MOOC series later on. And how do we approach the data. And then we have of course: why do we do all of this? In the end, we want to put all the pieces together and that is reconstruction simulation or simulation neuroscience. Another aspect is, which simulation neuroscience and large-scale industry mapping of the brain introduce is really what is called \"team science\". It is very different, it is where you have a lot of people with different expertise sitting around the table and solving one problem. Very different from the individual... ..that is sitting in the lab and addressing a specific question. It is not whether one is better or worse than another, they are all part of the whole approach that the world today is using to study the brain and they are all playing a specific role and they have their benefits in their own particular way. In many cases team neuroscience is indispensable. It has to be done that way and in many cases, you have to have individual research. Let's look now at the rationale. Why do we need to do simulation neuroscience? What is the goal of doing simulation neuroscience? We have experiments, we have theory. Many people say you only need theory, you only need experiments, you don't need to simulate the brain. There is a lot of debate and discussions and all kinds of ideas out there about simulation neuroscience. Why do you need it? Well, first of all, what we have to bear in mind is that actually simulation science is at the pinnacle of almost all the hard sciences. So you have got a material science in the beginning, you were tinkering around with chemistry and materials and you were building things, you were experimenting how materials work and how strong they are and how flexible they are and so on and so forth. Today, you simulate them. You simulate materials almost and you can actually simulate a huge repertoire of materials before you actually produce them. In engineering, there is almost nothing that is not simulated today. Toilet paper. Do you realize that you cannot actually produce toilet paper without simulation? These are very fine, very soft tissue that you have to cut in very specific ways and you have to roll them out at lightning speed in perfect... this cannot be done by experimentation. It cannot even be done by theory. You actually have to simulate the physics of the tissue in order to be able to wrap it. Diapers, the space shuttles. They have to be simulated in detail, all the electronics. Aircrafts, they are being built through simulations and then they are sent out to fly without even a test run. They just design them, simulate and they are ready to go. So simulation science is the pinnacle of most of the sciences. It hasn't arrived yet at neuroscience, but... ..well, it has arrived now at neuroscience. So, I want to give you the rationale, why do we need simulation neuroscience to understand the brain? The first and actually, the main reason is that understanding the brain is a big data problem. If you look at the numbers, you realize that there is no way that you are going to be able to understand all these pieces and how they interact together. You have 20,000 genes. About half of them are expressed in every cell in the brain. There is about 1,500 that are very sort of over expressed in the brain, but most cells express about 30% to 50% of their genes. Then they produce around 200,000 proteins, different types of proteins, but each protein has a variant. So you have about a million protein variants per cell. Then you have, probably, we predict about a billion molecules in a cell. There are thousands of molecular pathways. There are millions of molecular interactions per second in a cell. There at least 2,000 cell types. So we have about 900 brain regions. In each brain region, there can be... ..at a minimum of two types of cells, but in many cases 10, 20, 50 types of cells. So there is well over 2,000 different cell types. In the mouse brain, you have about a thousand kilometers of fibers. In the human brain about a million kilometers of fibers. In the mouse brain, about a trillion synapses, in the human brain about a thousand trillion synapses. Mouse brain about a hundred million neurons, in the human brain, about a hundred billion neurons. So just for one instance, one age, one species, one strain, individual variations and one disease, this is everything you have to map and understand how they all interact together to give rise to the emergent behavior, to give rise to psychophysical properties, to give rise to cognition, to give rise to a behavioral repertoire and so on and so forth. Imagine if you have to do it for all the ages, all the species, all individual variations, and for the more or less 600 different brain diseases. This is not a problem that we can solve only with experimental neuroscience. Experiments of neuroscience is key to ground truth. You have to go back and test. Always go back and test and experiment, but there is no way that you can understand the brain if you do not have a strategy where you can look at how all of these elements are interacting with each other. And be able to predict and look, explore how they could be changing the interactions in different states of age species and diseases. So it is a big data problem and there is no theory that is going to solve this whole problem for you in terms of understanding all the details of what is going on in the brain and there is not one experiment that is going to solve all of this. So for simulation neuroscience, the claim is: it is essential for an ultimate understanding of all the components of the brain and how they give rise to us, our personalities and behavior. \n ---------- \nOkay, so let's look now at some of the principles. What are the principles on which simulation neuroscience rests? So, the first important principle is that we have to do dense reconstructions dense, very detailed reconstructions and we have to do it from sparse data. Ok, so, I just gave you an idea, a glimpse at how much data there really is to measure. And if anybody has to do a calculation you will realize that it would take the entire human race, thousands of years if you wanted to map out all of those things in all of those different connotations. So, we will forever have sparse data. No matter how much data we collect, it will forever be sparse in terms of experimental. So, the problem we have to solve is how do we get a complete picture from sparse data. So, the first principle of simulation neuroscience is we have to establish the strategy from going to a little bit of data to using whatever knowledge we have of how the pieces fit together to build algorithms and algorithmically build the circuit. Now, this first principle is not a new principle. It is extremely old, it is the basis, actually, of our entire society today of information and communication technologies. Let me try and explain this in another way. Imagine that you have a TV screen Okay? And the channel is unplugged. So, it is a white noise. So, basically every pixel is independent of every pixel. Okay? And now I want to transmit this image to you. I want to tell you what is this image. I need to give you a complete picture of what this image is. Based on Claude Shannon's work from the 40s we know the theory here It is very clear. I have to measure if this is a random system. If this is a random image or a random system I have to measure every single pixel. That is I have no way out, but to measure every pixel if I wanna tell you about this complete story here Okay? But it is only if I have a random system. If I now have an organized system where there is a very clear structure the theory shows you absolutely clearly that I don't need to transmit every pixel to here, I don't need to transmit every single pixel, I don't have to tell you what every pixel is here because there are relationships between all these pixels and it is the basis of image compression. So, this is order. So, in this case if I have a random system I don't need any intelligence, I just have to copy these pixels across. So, I don't need an algorithm here. Okay? I don't need an algorithm, I just need to transfer the data. But if I have the order in the system I need a rule how to rebuild it if I am going to send a few of these pixels across and I want to rebuild this picture. I need an algorithm. Okay? I need some rule that will rebuild the relationships that are in this picture. Okay, so in a way with a random system I need to measure everything and I do it blindly, I don't understand anything, I just have to copy it from here to here. In an ordered system I have to understand order. And I have to package it into an algorithm. Then I can send it to you and I can reconstruct it. Basically, the important message is that dense reconstructing from sparse data is not just because we have to, because we forever have sparse data it is because it is science. It is actually not scientific to say that we need to go and measure everything in the brain. Okay? Only if it is a random system do we need to measure every pixel. You don't need any principles, you don't have to understand very much and you can close your eyes and blindly map one thing to the next. So, when there is an objection that says: \"Oh, you can't reconstruct or simulate a cell because we don't know everything about the cell\", that is not a scientific statement. The challenge is to find the minimum data you need. The smallest data set, not the largest data set. The real science is to say what is the least amount of data I need to be able to reconstruct it. That is a compression algorithm. It is a simile, it is the inverse, what is the least amount of data that I need to transmit to you so, you can rebuild the image. Okay. In that case you do not need to measure everything, you need principles and that is what drives simulation neuroscience. And then you algorithmically reconstruct it. So a simulation neuroscience is fundamentally about finding those principles, packaging them into algorithms, and then reconstructing. That is the first principle of simulation neuroscience. The second principle is, actually also quite similar to computer and software engineering. It is a bottom-up reconstruction process. And it actually has very strict rules. And the rules are, first of all, you have got to follow the biological principles. You don't try and invent how you think the brain should look. or the neuron should look. You measure and you use as much of the data that is available and where it is not available you have a hypothesis of what is available. And you can then test that hypothesis. You build the smallest components first you really start atomic, if you wish, you start at the eye channel to build a neuron, you don't start at the neuron and then try make a neuron behave and then say: \"Oh, let me go back and build eye channels\". You start at the very bottom and you build upwards Okay? And a key thing which is the same as what you do in computer and software engineering, you build modules and cells, and functions and there you freeze your variables and you freeze them. And then you combine them. We go to pi cell, it is so beautiful, it is combinatorial because I just built these and I can consume them later. But in order to do that I freeze the components and then I can combine the components. And the other important thing and very different from computation neuroscience, for example is you validate upwards. In other words, you validate what emerges when you combine things. You don't try to fit and your data, you never fit the data looking up at an emerging property. So, for example, you wouldn't try to say: \"look, my brain is not speaking how can I adjust my eye channels until I speak?\" You will be adjusting eye channels forever. Okay, that is a massively, it is an intractable problem. So, you have got to come down to the emerging property as close to that parameter as possible. For example, an eye channel, you want to build an eye channel you look at the behavior of an eye channel. You don't look at the behavior of the neuron to build an eye channel. So, you always do it, at the bottom you freeze it, and then when you say: \"ok, I have got my eye channels, I want to know what they do, I put them in a neuron and then I look at the neuronal behavior and that way I can see how they play out when they are being used. If you find that you can never replicate the neurons behavior then you can say: \"maybe I got my eye channels wrong. Maybe I don't have the right eye channels.\" And so on. So that is the way we begin the cycle, which we are going to learn a lot about in the course of this and other lectures. The guiding principle here is that if you build it right it will automatically behave right. OK, so you build bottom up. You build the components you freeze them. If this is valid in terms of the behavior even if you don't know all the pieces, but if a neuron behaves right, and you put in a circuit you can talk about what neurons do. If the circuit behaves right and you put it into a brain you can talk about what the circuit does and not the brain. OK, so you don't try and tweak the model to behave right. If the neuron, if you built it right it automatically behaves right. The last, the third, and there are of course many but the third principle is that it is iterative reconstruction and testing. So you get your experimental data, you get your literature, you go and find all the papers you can you mind all the facts you possibly can. and you try and build the unifying model of a neuron. You find the eye channels, you find the morphologies, the structures the physiology, any data that you have about that neuron you collect. And then the challenge is how do you package that into the simplest algorithm that will then reconstruct it that is consistent with all the data that you gathered? You obtain your model and then you interrogate it anatomically, you look at it and you see: \"OK, I got it right, it looks like a real neuron, I do the statistics between my reconstructed neuron and the real neuron and I see that they match\". I look at where the eye channels are, they match and so on. And then you simulate it to see if it behaves the way that it does in the real tissue. And then you do an analysis, you do the systematic comparison and you get your errors. Well, it is not going to work the first time. In fact, the goal is to do it very fast, fail very fast, identify where you may have got the wrong data or the wrong assumptions and then you build it again, and you keep going through the cycle as many times as you possibly can. In fact, it is an endless loop, you keep going and integrating and perfecting it, but with each cycle, this model replicates the tissue more and more accurately. You follow this process, you can't go left, you can't go right you can't go backwards, you can only go forward. So, the model can only improve each and every time that you integrate it. So, to illustrate this let's just imagine that you step inside a forest of neurons and you look up at the neurons, there are thousands of neurons millions of synapses, and you look around. And you say: \"I wonder how these neurons connect? I mean, how are they connecting with each other? Who is connected with each other? Where are all these...?\" So, how do you approach this problem? Of course you can try and let's take an electro microscope and measure everything that is, of course, one approach, but it you actually look at it you will be measuring for a very very long time But there is another approach and the way that we do it in simulation neuroscience, we say: \"OK, let's just look at what we know. Let's start off and let's imagine that we have a whole series of cells, neurons\". OK. And, of course, there are different kinds of neurons. You have many different kinds of neurons. Each one looks different, behaves different and it can, of course, connect differently to the others. So, how do you approach this problem ? Well, you can say: \"What do we know?\" And maybe there's only one experiment that tells you how this one cell, this one type of cell connects with others. There are potentially thousands of different types of connections here, but you only have one experimental data point. And this experimental data point you go and you see that the axon that goes to connect all these neurons does this. And you look at this and you analyze the data here and you say: \"Oh, I can see the principle\". First principle: The axon somehow touches every neuron. Okay? So, before I look at who is connected I just see that the way this axon goes, it is like what we call a tabular riser. It goes and it touches everything on you. And then I look at this one neuron and I say: Oh, I see that when it touches, when it touches onto another cell, it actually puts lots of little synapses on it It never puts just one synapse\". OK? It never puts just one synapse it puts lots of them. So, I see principle number 2: it is a multi synapse rule. Okay? And I think: \"oh that's interesting, but what else can I learn by looking at this one experiment?\" I see principle number 3: I see, wow, if I look at the axon of the cell, and I look at it very carefully I see that it has got places called blue tones which is where it can form synapses. So, I think: \"how many are there? And what is the space between them?\" And then I say: \"okay, that will limit how many neurons this one can contact\", right, because if I have an axon only as two then it is, of course, going to have a very different kind of activity than I have axon that has many boutons. Then now I have a bouton principle. It is a physical limitation. If I have lots of boutons, if I have a neuron with an axon with lots of boutons, then I can contact lots of cells. If I have one, I can contact one cell. So, these are 3 principles. Now there are more principles, but let's just assume, just say: \"OK I like these 3 principles, I am going to see if I can rebuild the tissue just with these three principles\". So, we package them into an algorithm. Okay? And digitally we reconstruct this tissue. Now we have 2 possible scenarios: it succeeds or it fails. Succeeds means that you have a look at it and then somebody tells you: \"Hey, you know what? There are 10 other experiments out there that looked at how this cell connects, and this cell connects and you have other data that you can check it with\". And then when you look at that you see that your reconstructed tissue now reproduces all of that experiment. So, you think: \"wow, I mean I looked at 1 cell, applied 3 principles of how they should connect with each other and they reproduce what other people have done in other laboratories all over the world\". That is a success. But what is important to understand with this success is that you haven't really gained any new knowledge, but you have gained a new understanding. You understand how your principles interact. You understand that this principle for this neuron applies to all of them. Two very concrete new understandings. It is not new knowledge because this knowledge existed, people had measured this, we saw this, it was just fragmented knowledge but you didn't even had that sort of integrated, unified view. So, when it succeeds, you understand better. I am an experimental neuroscientist and I have looked under microscopes for 20 years. And when I put this together, in a single reconstruction I understand what I was doing for all those 20 years. Because you can see this as an integrated, an integrated answer or perspective on what the pieces are that you have been collecting for so long. Now the other scenario is it fails. I do this, I check it again, somebody comes into the door and they say: \"Look, I have an experiment and it shows me that this cell, it really doesn't follow those principles and it connects in a completely different way\". And then I have new knowledge. I know that these principles are not enough. There have to be new principles. There have to be some other rules that will apply whether they are exceptions, or whether they are that these principles have to be modified. But there has to be something else that has to be understood. And then one can start investigating and that triggers, when you fail it triggers new experiments which you have to test now. How would an algorithm reconcile these differences? Another way to describe this is imagine that this algorithm works in one part of the brain. And it explains all the way the neurons connect. Now I go to another part of the brain and it doesn't work there. You think: \"that is terrible\"! No, that is great, because it tells you that in that other part of the brain there is something else going on. And that something else nobody knew about before. Because you had too much fragmented knowledge. That is when failing really leads to an advancement of knowledge. New knowledge. Because it triggers a completely new perspective. It actually shows you the boundaries of a current knowledge. You get to the end and you say : \"I've taken all the data, all the principles and I can't explain how this happens\". Then you are at the frontier. And at that frontier you can either do theory, you can do experiment you can iteratively test, but there are many ways to advance it. And this is a very exciting approach to advancing knowledge. Okay, so we looked at these 3 principles on which simulation neuroscience is based. \n ---------- \nOkay. So, in the last lecture we... ..explored a little bit what simulation neuroscience is, what the rationale for it is and what the principles on which it is based are. Now we are going to look a little bit about how we…, still part of this introduction, how do we... ..look, approach data, what are the data strategies and informatics and so on. So in the case of, you know... ..going just up to building circuits of the brain, the first approach is, establish a data hierarchy and there is a reason that we actually approach simulation neuroscience at the micro circuit level. It is because it kind of is in the middle between... ..brain regions and the whole brain and cells and molecules and so you actually capture how do you integrate the components below. Later you can integrate these components into higher level components. So, it is relatively simple rules for how do you approach data. You establish a data hierarchy. And in this case, it is also relatively simple. You start with your ion channels which are membrane proteins that allow specific ions to flow through and in either directions or whichever directions, and as they flow through, they change the voltage of the... ..membrane of the neuron and collectively they will shape the way that the neuron behaves and we are going to learn a lot more about that as we go further. The way we do that and we are going to also go further into that in the next series of lectures as we do ion channel screening, biophysical screening, one has to actually explore the biophysics of these channels and then one has to look at neurons and how we study neurons, how do we record from neurons and stay in neurons and how do we determine how they behave and then their synapses, how do we get to record how a synapse behaves. So these are paired recordings and staining the two neurons, so you can see how neurons connect with each other and then there are also circuits and how do you study circuits. You can stain them globally, you can do antibody staining, you can do stain for the expression of the genes, it is called in situ hybridization, or you can actually suck out some of the cytoplasm from inside a cell and you can find out which genes are expressed. You can also look at the emergent activities that emerge in the circuit electrically and voltage sensitive. So there are many techniques and approaches to record from the sort of levels, ion channels, neurons, synapses, circuits and then... ..in order to do the reconstruction, you have to... ...and build algorithms, you have to pay attention to the way you analyze the data. So you have to analyze in the case of ion channels, you have to analyze the kinetics of the channels, open and closed. Distribution - where are they located? In the cell body or up in the branches? What are the combinations of ion channels? That is you can't just have one type of ion channel, you need lots of different types of ion channels if you want to get interesting behavior. So, what are the combinations that are possible? And then how many of each type do you have? What is the conductance, how much current does ion channel pass through? So, this allows you to build a model of the ion channel and we are going to learn how we do that in other lectures later. And then you study neurons and we use patch-clamp and I will show you in subsequent lectures how we do that and that allows you to get the morphology, so you can get the branching, 3-D branching structure of a neuron to the morphology. You can also record from these, so you can actually find out how they behave electrically. You can stain them, so you can find out what proteins are located on to them And you can also suck out some of this juice from the cytoplasm, from inside a cell and then you can find out what genes are switched on in the cell, so you can find out genes. So it gives you a very in-depth view of a single neuron in terms of its structure, its function, its genes, its proteins and so on. And synapses are of course more tricky. They are small, they are half a micron. Basically, a little bit bigger than a bacteria, about the size of a bacteria for many of them. They are the most complex little devices, biological devices in the universe, but how do we record from them, so you have to record from two cells and then you have to stain both cells, so you can find a way for the axon to go out to touch the other cell and how did they form synapses. So you want to measure and the numbers you need are like synapses per connection, you need to stimulate and see how much voltage is produced on the other side by the receiving neurons, you get the synaptic strength and then you have to play with the synaptic transmission to try to understand what kind of dynamics, what is the communication code between two neurons. Synaptic dynamics and then you can ask, \"how does the synapse, how can it be controlled if you throw a drug on it or if you throw a chemical on it like a neuromodulator, (INAUDIBLE) or adrenaline, dopamine, histamine\". How does that actually change the behavior of the synapse. So these are all numbers that you can get and they are being obtained in many different ways, in many different labs around the world and then you have circuits and from the circuits you want to know essentially, how many neurons they are, how many types of neurons they are, how are they arranged together, how are they positioned, are they clustered together and so on. All kinds of geometrical rules of how do you organize your neurons in a piece of tissue. Then there is, of course also, electrophysiological data which tells you how that circuit behaves and many experiments around how little pieces of brain tissue or circuits behave. So this is, you know, to summarize the issue, identify a hierarchy. In this case it is ion channels. They go into the neurons, the synapses connect the neurons and you put the neurons together in a circuit. So these are the components. When you build your ion channels you freeze, you build your neurons, you freeze, you build your synapses, you freeze, you put them into the circuit and test. \n ---------- \nLet's look at the informatics strategies that we use or the principles of informatics. And as I said earlier neuroinformatics is probably one of the most challenging informatics sciences there are because of the data heterogeneity, the complexity, the lack of standardization, the uncertainty, different formats, there are many many problems in trying to database the brain. What is very important when you look at neuroscience data from a simulation neuroscience perspective is that, and you will learn this more and more as we go further, is that all data is good. There is actually no such thing as bad data. There is wrong data, there is a lot of wrong data, In fact, most of the data is wrong. But all data is good for reconstruction. Even bad data, even wrong data that says: \"Look, you know, this is just a wrong experiment\". When you have wrong data, it actually helps you understand better what is a right data. So, what we try to do is to build an infrastructure and Sean here is going to give you a series of lectures on this, we build an infrastructure that allows us to compare experimental data with simulation data. Because that is how we test and validate the reconstructions. So, you have experiments and your simulations and we try to make them as equivalent as possible. And then we want to informatically map them in a way that we can also compare them as well, as much as possible. So you want to easily be able to compare experimental data with simulation data. You need access to all the data, all the papers that is out there. There are around 10 million papers that are relevant to understanding the brain, not all directly on the brain but are relevant to understanding the brain. In some form you want to get all those papers, you want to mine them for little nuggets of knowledge that may be used, that you could use on the experiment side so that at some point you can use them to validate your reconstruction. So, you want to find all the papers and associated data. The open access and open data policies are extremely important if we want to be able to move towards simulating the brain, simulating the heart, liver, or any part of the body or simulating diseases. It is essential that data and papers are open. You want to organize them, so you have got to create knowledge graphs knowledge spaces, so that you can mine and analyze this data easily and ultimately, you actually want to map them onto an atlas. So that you can navigate in the kind of coordinate space that the brain is. You want to navigate through to different part of the brain and you want to say: \"What do I know about it? What are all the papers, what is all the data?\" And then I can see experimental data and I can see simulation data. And I can compare. And I can see how close is the simulation data getting to the real data. So, this is the general approach to informatics or neuroinformatics that we use. And again, the principle is: get all the data. Wrong data actually helps in some ways to be able to find out and understand what is right data. And I will give you many examples of this as we go on in the next lectures. \n ---------- \nSo, let's now go to reconstruction strategies. In reconstruction, I already mentioned that you build your components bottom up. So you start with your ion channels, you build your neurons, you build your synapses, you connect your neurons with synapses and you get circuits. You package them into workflows and to workflows and you automate these workflows as much as possible, because as I said, the most important thing in simulation neuroscience is actually to reconstruct, test, validate, find mistakes, reconstruct again and again and again and again until it becomes a digital copy of the tissue. So here is an example and we will be going over that in over many lectures. You have your neurons, you find out all your types of neurons and then you work out how to position them together and then you find out how to connect them together and then you add the physiology, how do they behave, every neuron behaves. And then how every synapse behaves and then you have a circuit. The last part that we will look at is simulation strategies and of course this involves billions and billions of calculations in order to compute every ion channel opening and there may be tens of thousands of ion channels in a single neuron and then there are currents flowing inside a neuron. So there are a lot of calculations and Felix is going to give you some ideas of the numbers of calculations and types of calculations that have to be done, but clearly you need a supercomputer. But sometimes people say \"oh, why didn't you do it on the grid?\" or \"why didn't you do it on some cluster or whatever?\" The important thing about simulating neural tissue, not building a model, (INAUDIBLE) simulating your tissue, a digital reconstruction is that... ...the whole simulation will run as slow as or fast as the slowest interaction. The neurons have too much memory to be on one processor, so each processor has to transmit information to another processor and so when the neurons communicate, they are actually communicating between processes and if you had a grid for example, and just one connection was slow, you would have to wait. The whole brain, the whole tissue that you have reconstructed has to wait for that transmission to occur before the rest can happen. So you need a supercomputer, where there is fast interconnect. So that is why you need a supercomputer, you need to have simulation code, of course that can distribute this problem of all these neurons across many, many processes and so you need the parallel code and in the Blue Brain Project we have... ..worked with the father of one of the most important simulators called NEURON, it is Michael Hines from Yale, and we worked with him to advance this code, so that it can really run very large scale simulations on supercomputers. So you need parallel code and then there are lots of other challenges. Supercomputers were designed and built largely for nuclear simulations and for other kinds of applications in physics and in particle physics, where you have a big problem, you ask a question and you get the simple answer back. It is not as simple. It is a very interesting answer, but you get an answer back and in simulation neuroscience, you want to... you are dealing with a lot of memory and you want to know everything that is happening inside the reconstructed tissue. So you need data intensive supercomputing and you need efficient algorithms to handle data. You need to balance the load. One neuron may be very big another one may be very small, so you need to know how to distribute your neurons across processes and you need to know how to manage all your resources, your computing resources. One may be going very slowly, one may be going very fast. So it is a resource management challenge, it is a data management challenge, but principally, you can summarize that as you need data intensive supercomputing and supercomputers are slowly moving in that direction where they can hold a lot of memory. Now the value of simulations, why do you need a simulation? Is it just that you get a nice pretty picture? Well, it is because if you build it, you want to see how it behaves and you want to be able to compare it to a real piece of tissue. So the way we see the reconstructed tissue is like a virtual brain or virtual piece of tissue and we want that we can visualize it and experiment on it as if it is a real piece of tissue. Now we look at real pieces of tissues under the microscope. We stain their cells, we measure them electrically and so we actually do very similar... we build in silico microscopes, in silico recording devices and so on, so that we can directly compare experiments with a virtual tissue. Here is just an example, where the difference when you have a reconstruction, a detailed reconstruction, compared to the biological tissue is that in the reconstruction, you know everything. You know every neuron, the name of every neuron, you know every single spike that is occurring, where the neuron is located, you know every synapse and so it gives you an x-ray view into the digital tissue. This is impossible in experiments. You can get a tiny small view into what is going on, but you can't get a global view and you can't get a deep view. So by reconstructing the tissue, if your algorithms are good enough to replicate the tissue, then you have a virtual exploratory environment where you have an x-ray view into all the mechanisms. And in subsequent lectures, we will see the power of that. That it can actually give you, take you way beyond what experiments can do. You can explore mechanisms, you can test hypotheses about how the circuit behaves in ways that are impossible experimentally. \n ---------- \nOkay so let's summarize by looking at caveats. So the main caveat of course is that there may be critical data that is missing. Now I did mention that the challenge or the goal, the mission of simulation beuroscience is not to get everything. It doesn't mean that we shouldn't get as much data as possible but the algorithmic goal is to find what is the minimum data I need to do a complete reconstruction. And in that search for the minimal data, it may be that there is critical data missing. If you don't have neuronal morphologies, it is going to be very difficult to build a circuit. So you have to find a way to get neuronal morphologies. So we can call them critical data or strategic data. And that means that one has to go back to lab and one has to do experiments and one has to obtain that critical data. The other caveat is that biological data has a lot of mistakes. In fact there are much more mistakes than accurate data. It is by far more that there are errors than there are correct data. So we shouldn't be so quick to believe biological experiments, I am a biological experiment, I measure lots of things and when you actually look at it and try to use it, you quickly see if it really fits with other data or not. In fact the best way to find out if your data is right or wrong is to try to use it in the combination of others. And we will show you examples where this actually does show you precisely where experiments have gone wrong. But it does make reconstructing difficult when you have this huge discrepancy in the biological data set. But on the other hand, it turns into a positive because you can curate the data, you can actually identify what is reasonable. This data is just not reasonable, it cannot fit with all this other data. And then the other caveat and it is not really a caveat, it just means that we are evolving in the way that we do Neuroscience. Instead of disciplines being fragmented across many labs and different people, if you do simulation neuroscience, you have to become multidisciplinary. You have to learn about experiments, analysis of data, informatics computer science, mathematics, algorithmics, computational sciences and so on. In summary, we looked at approaches. And it is really experimentation, theory and the third branch of knowledge generation is simulation science. Not only in neuroscience, in many of the other sciences. There are 3 branches of how you advance a knowledge. The rationale for simulation neuroscience is that it is a big data problem. It is not going to be solved only with experiments. It is not going to be solved only with theory. The principles of simulation neuroscience are fundamentally that you need to build a dense reconstruction from sparse data. Your challenge is to find the smallest data set that will give you a detailed reconstruction. You build bottom up. In other approaches, you start top down. Iterative reconstruction, you reconstruct, reconstruct and it refines. It is almost like a fitting algorithm or a gradient descent where gradually the digital reconstruction becomes more and more precise becoming the digital copy of the neuro tissue. The data strategy is really to establish a hierarchy of data components. You have to do a lot of very careful data analysis so you get all the parameters. Informatics strategy. Find it all. Bring it all on the table and ask: \"how does it fit together?\" There is some value in every study. And this is actually very powerful because if you go into the literature and you say I am only going to find the best work out there, you are going to have a hard time deciding what is the best work and there is going to be contradictions and not but more importantly, you could miss very important little statements, findings in bad papers. So, our approach is, let's find the gems in any paper. Some papers may have lots of gems and other papers may only have one and some may have zero but that is fine, you don't know that and you actually have to approach it as if there is a gem and I want to look for it. That is the informatics strategy that we adopt in simulation neuroscience. Reconstruction strategy, you build the components, you freeze down and validate up. You validate the emerging properties. You never fit to the emerging or you try not to fit to the emerging properties. You build workflows so you can run this machinery over and over. You validate against independent data and emerging properties. And, of course, you need a supercomputer because you need a lot of fast communication and you need a lot of memory and it does simulations and is about allowing you to mimic experimental neuroscience. \n ---------- \nOkay. In the last lectures, we learned a little bit about what is simulation neuroscience, the rationale behind it, some of the principles on which it is based and some of the caveats to keep in mind. And in this lecture, we are going to look at how we study single neurons, how we profile the morphologies and how we profile the electrophysiological properties of the neurons. So, this is an example of an experimental lab that tries to reverse-engineer or experimentally study the micro-circuitry, individual neurons, synaptic connections and how the micro circuit is put together. So there are a number of components here. You have what is called... ..an infrared differential interference contrast microscope. It is basically a clever trick you can look it up on the internet. It is a clever trick to get very high depth view into the tissue. So, it just cleans up some of the optical noise, but it is a beautiful technique that was developed by Hans-Ulrich Dodt and really pioneered in Bert Sakmann's lab in Heidelberg. We were all postdocs there at the time and we started doing a lot of experiments using this kind of microscope, because it gave us an unprecedented access and view into the neural tissue. There are these manipulators that will bring electrodes down into the tissue that is under the microscope. It is all controlled by pads and in fact, we even have a setup which we call fly patching, because it is all done as a sort of remote cockpit that controls all of these electrodes. You have amplifiers that amplify the electrical signal and you have a number of... you can put imaging techniques as well. These are controlling the manipulators, but you can also have imaging techniques, so you can look at fluorescence or any other kinds of images. It is just another example of lots of pipettes, the sort of record is 12, you can only record 12 neurons at a time. It is very few when you think about the whole brain, but you get very high resolution recordings. This is a movie that basically just gives you a little bit more of a perspective of... ..what the setup looks like and here is a movie that shows you what patch-clamping is. This is a glass pipette. The tip is about a micron in diameter or a little bit bigger and it approaches this neuron and you can see, it is patching the cell. Now what that means is that if you have a cell, you come in with a pipette and you come closer, you actually blow... you put pressure at the back, so it blows solution out from the tip and it cleans the membrane, so you can actually get the glass patching onto the cell and the secret of patch-clamp which was discovered by... ..Hamill and Bert Sakmann and Erwin Neher, and they won the Nobel prize for this, is that it forms a gigaseal here, so very high resistance. It is very high resistance between here, so the current will not leak out of here and it means that you can break this membrane and what you end up with is... ..your electrical recording here that you can then measure the voltage inside and as you should know, these are basic things you should know, inside is negative compared to the outside, so this is typically about minus 60 millivolts and then you can see fluctuations in this voltage. So this is called whole-cell patch-clamp. Whole-cell patch-clamp, and there are of course other configurations that you can also do to get a piece of the membrane and so on, but I won't go into that. So this is whole-cell patch-clamp and it gives us access to a single neuron and with this access you can inject a dye into it and here is a movie just showing you what that cell looks like after we have injected a dye into that cell and stained it like developing a photograph and then you get a full picture of... ..what that morphology of that neuron looks like. You can also inject fluorescent dyes into it. You can inject calcium sensitive dyes, you can do lots of different things, injecting all kinds of chemicals and experimental agents into cells and then you can see them. You can of course also record from the dendrites, you can record from the cell body, you can record from the axon and there are many studies that you can look up about this, because a lot of these studies are used in the reconstruction. And that allows you to really get a very high resolution view of nudges, the morphology, but also the electrical behavior of a neuron at the cell body in the dendrites or in the axon. These are axons going down. The sort of repertoire that you can play out here as you obtain a whole-cell, this is a whole-cell patch clamp here, the pipette is coming in here too. It has broken the membrane. You have a gigaseal, so you can record from the cell and this is what an electrical behavior will give a pulse, if you give it a step current pulse into the cell and the cell will react by producing action potentials. It is producing one and then a burst of action potentials. It stops and then there is another burst of action potentials and this, I will show you how we profile this, so that you can... you do a lot of testing to find out how does the neuron react to electrical current that comes in. You can also suck out some of the... we call it cytoplasm harvesting where you take out some of the juice, the cytoplasm from inside the cell and you do... you isolate the RNA, the messenger RNA. You can convert that to cDNA in a process called reverse transcription and then you get cDNA and then you can fish. You put in little probes with sequences of specific genes and you can fish out different genes. You can ask yourself \"I want to find out, if gene X, this purple one, is inside the cell\". You make a specific gene sequence and you send in this probe and it tells you whether this purple gene is inside this cell using a process we call multiplex PCR and today there are many more approaches to find out which genes are expressed inside a cell. You can load the cell with a dye as I said before and then under a microscope, you can reconstruct it and here in red you see the... in red you see the dendrites going in different places. You draw this in three dimensions under a 3-D microscope with an XYZ stage. So, you are actually tracing it in 3D and then you can also trace the axon and that gives you the axonal hover of the neuron. You have to do this for many cells. They take a long time. It takes a week or so or a few days at least to draw, trace one cell in 3D, so you have got a digital model of the cell and in our case, you know, we have about a thousand of these cells reconstructed which is not, as I said in the first stage, it is still sparse data, but that took 20 years to obtain, so you have to find other ways to build up your data set. This is just an example where you can see the genes, the bands. There are techniques that you can use to see which genes... These are ion channel genes. Names of different ion channel genes and it tells you whether these ion channels are the gene that they must switched on inside the cell. So this gives you a genetic profile or a molecular profile. This is an ion channel profile. You can also look at calcium binding proteins or neuro peptides so that you can actually identify the molecular profile of the cell. So, this is what you can do all in a single cell. You basically get its electrical profile, morphological profile and a molecular profile. \n ---------- \nOkay, so how do we profile the morphologies? Well, when you do this many times you of course see a vast number of cell types and here every cell type is colored a different color and there are many different morphologies different ways that the dendrites are branching different ways the axons are branching and so, you need a lot of examples of each of these cells and here is sort of a catalog, a sort of a periodic table of the main morphological cell types that are in the neo-cortex. These are inter neurons, the inhibitory inter neurons and these are excitatory, typically parametal cells but there are also other types of cells that are not strictly parametal cells, but excitatory cells. You can see they have many different branching features they have names, Martinotti cells Bi-tufted cells, Double-bouquet cell Bipolar cell, and so on, and these names actually come from about a hundred years of anatomical study starting with Ramon y Cajal, a very famous Spanish anatomist who actually drew lots of cells. Under a normal microscope he would actually look under the microscope and then he would just draw them. Today we can trace them in 3D but he named many of these cells and many of the famous anatomists that followed named many of these cells. Once you have the morphology, you can analyze this morphology and we have released an open source tool called Neuro-M which you can actually just run and analyse these and you can get lots of parameters about the cell how big the cell body is, how big the arbor is how frequently they branch, what the angle of branching is and so on, and then these features you can use either to try to classify cells to see which class a cell fits into but you can also use them in other ways to build new cells, and I will show you that in the moment. So, the important thing here is that the classification is based on local morphology so it is just on the way that this cell looks. This of course is not everything about a cell. Cells may project to further places but it is typically the excitatory cells, they project to different brain regions they project... whether it is across the hemisphere or to deeper brain regions or even just laterally to different parts of the neo cortex, but we just look at the local morphology and the name is based on the local morphology. Now one of the caveats in getting morphologies in slices, brain slices, you saw the slice before, is that you actually shave off in the slice because this is the slice, that is the thickness of the slice, so, you shave off a part of the neuron, the branches of a neuron. So after you have done that, you have to kind of repair the missing part and there is software called Neurorepair which identifies where these cut points are and then it grows these extended parts to be consistent with the statistics of the other side. This is how we repair the morphologies. Sometimes we also use data from in vivo recordings to learn how to do this repairing process better, so in vivo validation. One needs a certain number of cells to be able to clone them and make a bigger data set and here what you are seeing is the averages. The average is many cells overlayered on top of each other and you see a sort of an average hexagonal structure. This helps us obtain the statistics that you need to, A repair the cells. Also, you can now computationally clone these cells you can make many copies of them within the statistical variations. They are not exact copies, they are statistical copies so even in this case of the neo-cortical micro circuit we have about a thousand cells, but we have enough initial statistics, past data statistics, to actually build hundreds of thousands or millions of copies of each of these different cell types so we can produce many, many copies. That is one approach and a more recent approach that we are using, and we won't go into that in this MOOC, is where we actually computationally synthesized them using the rules of how they branch. The idea here is that you want to learn how to synthesize any cell type in the brain, and that is a long term goal. \n ---------- \nLet's look now at the electrophysiological profiling, how do we profile the electrical behavior of neurons. So, as I said, you obtain these wholesale recordings from cells. So you find out, you stimulate a positive current into this, so you get a step pulse here. And then you see how the cell reacts, in this case, it's producing a burst, it stops, and then fires more spikes. Here you do a high resolution, it is a short time frame, so you can actually look at when does the action potential start, you can get the amplitude, you can see how fast it is rising, you can see how fast it is falling, you can see how deep it goes after, because some can go... afterhyperpolarization, can be very big, some shallow. You can give it a ramp pulse to find out what is the threshold, what is the threshold of the action potential, when does the cell start generating action potentials, and then you can give a range of different current pulses to see how the cell will react to different current pulses. You can try to give a series of strong pulses to see whether the action potential will change, in some cells, the action potential drops and gets very small, in other cases they don't. So the amplitude can change, and those old properties... You can give short pulses and look what happens afterwards. And you can see here, that the cell hyperpolarizes and it recovers slowly after that. So this is an afterhyperpolarization potential. You can give it a negative pulse inward, and then hyperpolarize the membrane very fast and then it relaxes back. And you will see in Idan Segev's course that this can be used to calculate a lot of properties about the membrane. Then you can also give different pulses below thresholds, sub-thresholds. And you can calculate the input, the IV curve, the current-voltage relationship of a cell. So the idea is you do a whole battery of stimuli, to actually probe a neuron and see how it reacts. It is a bit like a psychologist for neurons. And then you classify the electrical behavior. So, when you do that, what we have found is that there are essentially eleven types of behaviors. You have cells that are slow-firing cells, for example, here or here. You have very fast-firing cells, they can go at high speed, these are fast-spikers or non-accommodating, classical non-accommodating cells. You have cells that, when you excite them, they say: \"wait a minute\", and there is a delay, and then they start spiking. And you have the opposite, where the cell starts with a burst so it starts actually with a very high frequency burst, and then it slows down. So you have all these different ways that a cell can behave, some stutter, we call them \"stuttering cells\", like here, that is almost like a morse code. Stuttering cells. Others are even called \"irregular spiking\", it means you can't actually predict when the cell is going to fire. You stimulate it and then in some other process they just fire whenever they seem to want to fire, it is not as predictable as any of these other approaches. One thing that we found is that for every morphology, you can have multiple different ways that they can behave. So you can get a certain morphological type of cell, and you can discover that same type of cell can behave in different ways. So either a fast-spiking cell, this is a burst irregular-spiking cell... So we call these morpho-electrical types. And so, in combination, you can try and map the morphology with, \"for this given morphology, which electrical types do you find?\" And you can create a map between different morphologies, these are morphologies and the different electrical behaviors. And what that gives you is, in the case here, where you have fifty-five morphologies, and you have eleven electrical types. In combination, it gives you 207 morpho-electrical types. So these are the individual characters, if you wish, of the circuit. \n ---------- \nWe briefly looked at how you study single neurons, we looked at how you obtain the morphology, how you could analyze the morphology, there is a lot more that you can read up on morphometrics, there is open source software that we have released that you can use to analyze the morphologies. And we looked at how you record from cells, obtain electrophysiological properties, and how you characterize them, and how you can use that to identify different behaviors. There is a lot in the literature and a lot of publications on how you classify based on these properties. It is not all clear sometimes, that it is a clear class, but there are approaches to be able to objectively classify, semi-objectively classify, and sometimes very subjectively classify cells. It is a big area of research to decide what really are classes, but in many cases, you have to look at multiple lines of evidence in order to decide what a class is, or whether there is a class. Lastly, let's look at just some caveats, and here that main caveat, of course, is that you have to get such high resolution data, you need to record from brain tissue, and brain tissue means you have got to cut the tissue, and that means that you have cut neurons and you have to find a way to repair these cut parts. With patch clamp, of course, which gives you the highest resolution view of the morphology and the electrical properties and molecular properties of a single neuron but you can only record from a few neurons. That is one limitation of it. So this really gives you high-quality spa state. And then, you have to classify. There is a whole field of science based on the classification of different morphologies and electrical types, to decide what they are. In summary, we learned about brain slices, that that is where the source of a lot of very detailed, high-resolution data comes from. We learned a little bit about patch-clamp, but, again, I encourage you to go and read up about the electronics, and the biophysics, and the electrophysiological principles behind it. We learned how to obtain morphologies. And we learned how to obtain electrical properties. \n ---------- \nWe looked at studying single neurons. Two different ways that we can explore neurons in terms of the morphology and electro-physiological properties And today we are going to look at ion channels, profiling, molecular profiling, what do we mean by neuron types and how do we actually obtain neuron densities and then some caveats in a summary. So ion channels, they are the heart of neurons if you wish or heart of the electrical activity of neurons and it is quite a challenge when you first go in and look at all the ion channels, but the organization is actually quite elegant. There are a lot of ion channel related genes, but many of these associated proteins that will regulate ion channels or that make ion channels work. There are about 280 well characterized ion channels and of them, 145 are voltage-gated which means that when the voltage changes, they either open or close. Now, the way to sort of make it easy to digest is first of all, everybody should know that ion channels, they are special transmembrane proteins that allow, selectively allow ions to flow through. You can have an ion channel that selectively allows sodium to flow through for example or specifically calcium or you could have typically potassium and what you should all remember and you should look this up is that the potassium of course, is very high on the inside of cells, sodium is very high on the outside of cells. Okay. Calcium is very low. 60 nanomolar or so on the inside of cells and calcium is actually relatively very high outside of cells in the order of 1 to 2 millimolar. Calcium is in the range of 60 nanomolar. There are pumps in that and we are not going to be considering these at this point in time. It is not... something you can get into later. So when you look at the potassium channels or just the channels in general, you have the sodium channels, that is over here. They are the ones that essentially, help you to get action potential, spikes. You have the calcium channels which also get activated. For example in the heart, most of the action potential starts off as a sodium channel and you have a long calcium action potential and in neurons, the sodium spikes help activate calcium channels which brings calcium into the cell and that drives a number of other biochemical processes. So this is how you get your excitation. It is with your sodium and calcium channels and then you have your resting membrane potential. Most cells as you should know inside is between 6 to minus 60 to minus 80 millivolts inside the cell and the way you maintain that is by keeping a lot of... ..potassium inside, but essentially, it is about the permeability. If a membrane that is only permeable to potassium channels, then you will go to what is called the potassium Nernst potential which you are going to learn about later on. And that keeps the cell at this potential. So you have to have sort of leak channels or channels that are just permeable, they are not... ...necessarily very sensitive to voltage, they just stay open all the time and they actually help keep the resting potential. So that is these. These K2... and the inward rectifiers. By convention when current flows out, positive current flows out, it is called an outward current and when positive current flows in, it is called an inward current. Okay. So the resting membrane potential as I said here is controlled by these channels. The depolarization is controlled by these channels and then you have hyperpolarization. What stops the excitation and brings it back down again, it is this class of channels, the voltage activated... ...potassium channels and you also have the calcium-activated potassium channels. So these are very important to bring the membrane potential back down again, to stop the firing and to allow another spike to occur. So if we focus a little bit on the voltage activated potassium channels, because they are really the extremely important in controlling excitation. You block them, you get seizures, everything goes wrong. There are 40 genes, only 40 genes in mammalian tissue and many of them are also similar in plants and others. In 12 classes, they are called Kv 1-12 and each gene produces a specific subunit here and you have to combine four of them together to form a channel. If you combine, if it is one gene that produces all four subunits, the same subunit, then you have what is called a homomeric channel. But you can have two genes, each producing two subunits and when they combine, you have what is called... ..a heteromeric channel. Homomeric for the same, heteromeric for different subunits, when different subunits combine. If it was only homomeric channels that could be produced, you would have 40 channels, of types of potassium channels, but because there are different combinations they can combine, different subunits can combine differently, the result is that there is well over 40 different voltage activated potassium channels. This is very important, because depending on which one you use, you are going to have a different electrical property in neurons as I will show you later. So these are the family of... the Kv family, what we call the Kv family. How do we characterize how these ion channels behave? One way is to isolate the messenger RNA. To isolate messenger RNA from the brain, so you basically squash up, homogenize the tissue, you isolate the messenger RNA, you do PCR amplification, you get your cDNA, you clone the gene and put it into a vector and you inject that into cell lines. So you produce cell lines. Cell lines like a Chinese hamster. Ovary cell lines or hex cell lines or different cell lines that you can use and then you can inject the ion channel gene into these cells and then they express, they express the channel on the membrane and you can induce the expression with tetracycline or other kinds of chemicals when you're ready to study the channel. And in the past it was that you needed to use patch-clamp. Patch-clamp, so you come in with a patch pipette. A very high resistance patch pipette here as I mentioned yesterday. A high gigaseal. Then you can actually see this channel opening and closing. Opening and closing. But today we do this in an industrial way. There are robots and basically, the robot just puts the pipettes in and patches automatically and so you can do this at a very high throughput way. Now what you do when you have your channel and you can study how it is opening and closing, you stimulate the channel. Here you can see that you give different pulses of different amplitudes and you study activation. How does the channel open when the voltage changes and you study in activation which is how does the channel close once it has been activated. So the voltage is active, but the channel just starts closing. Even if the voltage is high, the channel starts closing. So you can see here the channel starts closing. And then you have deactivation. Deactivation is, after it has been activated, how long does it recover from this activation. And then there are other dynamic variables and different ways that you can study the channel, but the most important is activation, inactivation, deactivation. So when you do that on the 40 ion channel genes, there are many that are silent, because homomeric subunits, when they combine, they are not going to produce any current. For example here, you are getting silent channels. These are silent channels. But there are many that activate and the first thing you can see is that they have different kinds of activation. Many different kinds of activation. This is Kv 10.2. It has a slow activation. This is Kv 1.4, very fast activation and inactivation. This one has almost no inactivation. It just keeps being activated. So opening, you can think of it like this that opening during a voltage change, you can either be fast activating or slow activating. Closing while the voltage is on, you can have fast inactivating or slow inactivating and then how does it recover after or close after activation. Fast deactivating, slow deactivating. So you can see now if you combine, the combinatorics of combining this can give you a rich, a very rich diversity of ion channel kinetics. Now, this rich diversity of kinetics is very powerful, because if you combine them in different ways, you can produce neurons with many different electrical behaviors and properties. Now unfortunately, at this stage, there isn't a comprehensive data set of every channel kinetic which we can introduce into a cell and we are building this and it will be completed in the next few years. In the meantime, what we do is we look at classes with build models of classes of channels and we just call them transient channels or persistent channels of calcium-activated and these are basically engineered. So they are engineered channels. They are not genetically determined channels. They are theoretical channels in a way, but we are replacing these channels and then you have a neuron that is very accurate. What is important in relationship with what I told you about yesterday or in the previous lectures is that, you can do this and then still fit the electrical behavior of the neuron, but what you won't be able to do is tell what is the significance of the ion channels. The only purpose, the ion channel serves is to give you neurons that behave the way that they behave in the brain and then you can use those neurons in circuits and you can tell what neurons are doing in circuits, but not what the ion channels are doing. So that is what limits your interpretation of your... ...simulation results. Here is Channelpedia. You can go there. There is a big database of all the channels, giving you literature, telling you about the channels as well as the models and there are Hodgkin-Huxley models and Markov models that you can download. You are going to learn how to do this in later courses. How to build the Hodgkin-Huxley model. So the beauty of having many different types of ion channels is that it depends on how you mix them. You can create voltage shapes. All kinds of voltage shapes. You can create bursting behavior and you can shape the way the neuron is going to behave and here is just an idea showing you that, you know, depending on what kind of sodium channel you have, it will determine how fast the voltage rises. It will determine whether the action potential gets smaller with each successive one or stays the same with each successive one and depending on the voltage activated potassium channels, it will decide on many things. How fast you return, how deep the return is, what is called the afterhyperpolarization potential and whether a new spike can occur or not occur. So all these channels, they really serve to control many features of the way the voltage is going to change. \n ---------- \nNext, let's look at the molecular profile. We will come back to the ion channel profiling and see how that relates to the way that the neurons are constructed. Let's briefly look first deeper into the molecular profile. So here for example, we started off, you remember we said how do we study neurons. We can do whole-cell patch-clamp, we get the electrical properties, we get the cytoplasm and we can determine which genes are expressed and we stay in the cell and we can get the morphology. Now what you can do is, and we can characterize the electrical properties. So you can say electrical property one, two, three, four, five and you can make a huge list of seventy or hundred electrical features such as, the speed of the action potential, upswing and downswing and amplitude and so on and then you can also take these molecular profiles which genes are switched on inside the cell and you can create Gene 1, Gene 2, Gene 3. And when you have that, you can start trying to compare and ask yourself: \"If I look just at the genes, how does it relate to what kind of electrical behavior it is?\" So here you see an example in this blue cell. These are the genes that are sort of negatively correlated, which means that it is highly unlikely to find these genes, it is highly unlikely to find these genes in the blue cell, but it is highly likely that you will find these genes in the blue cell and here it is the opposite. So these two neurons, the one has a delay, the other one has a fast start, they are both fast spiking cells, but they essentially have almost an inverted gene expression profile. So they are using almost exactly the opposite set of ion channels to produce their electrical behavior. And this is one of the information sets we use when we try and classify neurons. So we don't only look at their electrical properties, we also have looked deeply into what genes are expressed in them and that gives us confidence that we have two different classes. As you can see in this case, it is a very clear separation of two electrical types of cells. So what you can do is you can take this and this is a very simple linear operator where you take your electrical properties going from E1 to E61 and your genetic or ion channels, Kv 1-1 all the way down to even calcium binding proteins and other kinds of proteins that are in the cell. And you can ask how does each value correlates with each gene. And there are a lot of statistic you can do to see if it is statistically significant and establish an operator like this which gives you the measure of the correlation between these different parameters and as I said this is a very simple linear operator. You can build much more sophisticated nonlinear operators and multi dependency operators and so on, but just this linear operator is already very powerful, because here you can see that you can actually predict, you can look at the genes that are switched on and you can apply the linear operator and you can calculate with a very small error, the size of an action potential, the width of an action potential, the fast down swing of the voltage and so on. And the reason why this is extremely exciting and important is it means that ultimately, we won't have to record from every neuron in the brain in order to work out how it will behave. We can get the gene expression. So this is why one of the big goals of working the Blue Brain Project is to work with many labs around the world to try to obtain the single cell gene expression profiles. So if you go for example to the human brain or to a nonhuman primate brain or to any other species, you don't necessarily have to restart everything. You have to do some experiments to see that this holds for example, also in another species, but then you can actually try and get as much genetic information as possible and then you can predict the electrical properties. So that is an example again of how you constantly look for ways to rely on less and less data. If we had to rely on recording from every single type of cell in the brain in order to find out what electrical properties they had, it is going to take us a very, very long time to understand the brain. This is a way around it. It is not yet perfect, but it is the beginning of... ...a step in that direction. So in general, you can see that what determines, what determines the electrical properties of a neuron is essentially the kinetics of each channel. It depends on which combination-- if you combine this one, this one, this one and this one, you are going to get a certain behavior and if you combine this one and this one and this one and this one, you get a different behavior. So it depends on this combination and it depends on the specific kinetics and then it also depends on the location. Where is this channel located? Some channels are located only close to the cell body, other channels are located much more out in the dendrites. Other channels are located more in the axon and so if you know what the kinetics are, the combination of kinetics and you know where they are, you are very close to doing very little to simulate a single neuron. You are missing the densities. You are missing how many of these channels do you need to put in in each compartment. And that is something that you are going to learn about and actually run an exercise using blue pipe up to adjust the densities in order to reproduce electrical behavior in a neuron. So here is just an example again. The goal here is to get to densities and in this movie you can... it is just trying to illustrate how distributing channels differentially on dendrites can also give you many of these different electrical properties. The molecular diversity can be mapped, so you have your morphologies over here and you can also map them to the electrical properties, so you can produce this kind of map, which tells you how many combinations you can have which we also spoke about previously, but what is exciting about this is that you can continue this map. You can say: \"I have this kind of morphology, it expresses this these proteins\" and in this case, it is expressing two different kinds of proteins and it will have one, two, three, four, five different electrical behaviors. So you have this cell expressing these two proteins and it has five different electrical behaviors and you can build a map like that for each of these cells, in terms of what is the probability that they are going to express these different proteins. These are calcium binding proteins. They bind calcium and buffer calcium and these are neuro peptides and they exert other modulatory effects on how the neuron functions. It is not critically important to remember them. It is just that you have many, many proteins and peptides. And we choose these, because they tend to be characteristic of some types of cells. So with this map, it is possible now to actually go and stain your model neuron, because you can calculate the probabilities that they are going to have these proteins, so you can stain them. You add these proteins probabilistically into these kinds of neurons for example. And that way you can stain all your neurons, in silico neurons, and then you can of course compare them to what is seen in vitro, in the real tissue. So with that, we can now have a look at... you have a basic idea of the morphology profiling, electrophysiology profile, ion channel profiling, how we do that, molecular profiling in terms of other types of proteins such as calcium binding proteins and neuro peptides. And now let's briefly look at neuron types and there is... It is quite a huge field. Debatable. There is no clear consensus about how to name a neuron, how to classify a neuron, but there are some important guidelines and the guidelines that we use when naming a neuron is a bit like in chemistry. You start off by... ...identifying the location of the neuron in the brain. It may be in the visual cortex, in the layer two, so you would call it the visual cortex layer two cell. And then you identify the local morphology. So it may be in the visual cortex layer two, Martinotti cell. And then you identify what subclass of electrical behavior it has, so it would be visual cortex, layer two, Martinotti cell, delayed accommodation. And then you would identify what molecular profile it has, such as expressing somatostatin and then you would go further and indicate as many genes as you can find within that cell that may characterize that type of cell and then ultimately it may be that two neurons that are almost identical, they will not be genetically perfectly identical, but they may be very similar, they could actually project to different brain regions and you could classify these according to the projection type. So the way that we look at neuron classification and naming is that we should think about the dimensions in very separate ways and combine them in a systematic way starting with location to morphology to electrical properties, molecular, genetic and then projection, projection subclasses. \n ---------- \nLet's briefly look now at how we get to neuron densities. We are going to come back to this much later in the next MOOC actually about how do we reconstruct the circuit, but I just want to show you how that is used, how we use the molecular profiling to actually get to neuronal densities. So here is a movie that you can see that is showing stained cells. These are cell bodies. Actually, it is staining everything, glia and neurons, but then you can stain, you can apply different stains many different stains. So you can stain for the glia, you can stain for GABAergic cells and you can stain for neurons, just neurons in general and you can do many other stains and using these different stains, you can then start identifying how many neurons there are in each of the layers of the neocortex and these are some numbers and this is in the cell paper that you can you should be following during this course. The important point to make here also is that counting how many cells there are in a particular brain region is very unreliable. It is the most unreliable, probably one of the most unreliable data-sets you can get to. If you read a paper, tells you how many cells there are in the brain or in a particular part of the brain, just bear in mind that there are 20 other papers that will be contradicting that data. The most reliable way to do this is to do block staining. A big block and then you use clarity, so that you make sure that all your neurons are in that piece of tissue and there is a lot of certain care that has to be taken to make sure that you have controlled for shrinkage and many other things, to make sure that you really do know how many neurons there are in a particular part of the brain. Okay. So, here is just a movie, just showing you that you put them all together, we are going to come back to this much more in the future. And here again is... you have the in silico staining of each of these different peptides. It is not important what they are. It is just that you have many different peptides and each of the neuron is stained according to those maps that I showed you before and then you can do... so you stain your virtual tissue or the reconstruction and then you can have a look at what real tissue looks like when it is stained with the same peptides. And then you count and you look at the distributions that you see with in silico and the distributions you see in the real system. So then what you find if you actually... after you have reconstructed the circuit with different cells and different layers and then you find that it actually correlates in terms of the recipe which was obtained from getting those densities. It is one different data set and the actual measurements obtained from the real tissue. So the reconstruction then basically, looks like the real tissue. So if you were an anatomist and you were staining tissue and you stained the virtual piece of tissue, you would see the same patterns of staining as you do see in the brain tissue. \n ---------- \nThe first important caveat is that when you have heteromeric channels, the combinations are vast which means it is very unlikely we can measure them all, so we do need to find indirect ways to determine the kinetics of all the heteromeric channels. The second caveat is that to obtain very good ion channel data, kinetic data, it needs a lot of standardization. So you will find a big discrepancy of ion channel data in the literature, and it is largely due to different standards being applied in different laboratories. The third caveat is that classifications is still a murky ground, to classify different neurons electrically, morphologically, molecularly... And ultimately we would need single cell gene expression patterns, what we call single cell transcriptomes which is close, but it is not yet a solved problem. So, to summarize, in this lecture, you learned a little bit that there are many different ion channel genes, that each produce different kinetics, the electrical properties of a neuron is determined by the kinetics of each ion channel, by the combination of ion channels that you put into a neuron, by the location where you put them, and the densities of each ion channel type. If you know how the ion channel genes are expressed in a neuron, you can actually calculate electrical properties. And you also learned how to obtain the density of neurons, or at least got an idea how we obtain density of neurons. Okay, that is it for today. \n ---------- \nIn this lecture, we are going to look at synaptic connections how do we characterize a synaptic connection and briefly look at what stochastic synaptic transmission is and some caveats. So first of all before we go into looking at synaptic connections it is important to understand the location of synapses. Where they are is extremely important for what the cell body will sense and hence react when it sends the input further. So, the location is a very important feature and it is not just enough to say neuron A is connected to neuron B. In the case of detailed morphology the location is extremely important. And here what you see is experiments very elegant experiments done by Stewart and Sakmann, where they put an electrode into the cell body, and then an electrode into the dendrites and they evoke this action potential in the cell body and it traveled backwards, because we can see the delay and then you see it in the dendrite after so it basically travels back and it is called the back-propagating action potential. This is very important because this is a signal that mediates plasticity and many, many other things and it is a very important part... a lot of learning theory today. So, this is a key experiment that is related to the current understanding of how neurons learn but it also illustrates that if this propagates back with a delay it is going to hit different synapses at different times so synapses that are sitting at different places could be effected in very different ways when action potentials propagate back. This is another example if you had an input here in the dendrite and you inject a sharp current in the dendrite then this is what you see in the cell body. Iran Segev is going to give you all the theory and the mathematics behind what happens from here to here but for now what is important to understand is that if you had synapes here it is quite different from having a synapse here, so location is important and this rationale is important for how your characterize a connection. In the neocortex, there is actually only one connection out of thousands that could exist, that has been fully and extensively characterized and even in our lab we only did it once and then we did it with less and less data subsequently so this is this paper which is a comprehensive characterization and I will use it to illustrate what would be a sort of a comprehensive characterization of a synaptic connection. So you record from two neurons, this is your one pipette your second pipette, and then this is the action axon going from one cell, and they have collaterals which you can't see here, and these collaterals move up and they touch, they form synapses on this other neuron, and when you excite this cell you produce an action potential then in this cell you see this response occurring and every time there is an action potential you can see sometimes it is a small response sometimes it is a bigger response, and a bigger response and so this cell is sending a message to this cell but it is receiving it with a different delay and latency which is here, it takes about one and a half millisecond for the information to get across to the cell, but there is a distribution and it also has different amplitudes so stochastic in its sense is it has different amplitudes and different latencies and we are going to explore why. You can do a much more higher resolution view where you have what we call a pre-synaptic spike and then you can look at the response and you can characterize exactly this delay across many different connections. Previously it was just in one connection you can see these delays but now you can see across many connections and you can see there is a lot of range. So, some cells will take one millisecond to communicate with another cell other cells may take six milliseconds to communicate, the same types of cells, we got this big range of time delays between neurons. Some responses will be very short and some responses will also rise very slowly so they could go very fast up and down where they could go slowly up and slowly down. The amplitude also varies a lot, up to six actually we have even recorded even up to about 12mV for this kind of connection but most of them around 1mV range. So, typically, when one neuron connects to another neuron it excites it, it is producing a 1mV depolarization, what we call an excitatory post-synaptic potential. We can go a little bit further, and when you study these carefully, so you are triggering the spike here and you look at the response and then sometime it fails, nothing happens. If you look at the amplitudes that you get you either get nothing or you get a distribution so you get a range of amplitudes that can be generated inside a single cell but then you have many connections where most of them don't fail. So, this is rare for this type of connection where most of them don't fail. So, this is rare for this type of connection but there are many others that fail and some of them can have very high failure rate so they try to communicate but nothing is getting through. Here you can see an example, also, of a very strong connection, it is about 6.4 - 6.45 millivolts, but even on each trial there is a change in the amplitude and this is the range of the amplitude, as you can see. a change in the amplitude and this is the range of the amplitude, as you can see. So, when neurons communicate they communicate with a jitter a range from 1ms to 6ms, and the amplitude can range anything from 1mV to 12mV, and we need to understand those mechanisms and how to recreate those mechanisms. Okay so when you stain these neurons as we explained before how you stain them, you load a dye into these cells, this is a photograph under a microscope when you have developed this dye like a photograph, so you can see these cells. You can follow the axon and their collaterals here you can follow them, and you can identify where the axon is touching the dendrite of the other cell. And these areas are what we call putative synapses because at the light microscope level you can't identify whether it is real synapses or not, you can just say that because the axon is passing by and touching a dendrite and the axon is forming what is called a bouton, it is very likely that that is going to be a synapse. But this can be verified by looking and doing an electron-microscopic analysis and here you see the pre-synaptic side with vesicles, these are synaptic vesicles you see here, and this is the cleft. It is very difficult in experiments like this to get very high quality electro markers, EM pictures, and this is the synapse this is the post-synaptic side. So you can verify that, what we call physical touches are actual synapses at the electro-microscopic level. Now, what is important in this is that when we did this analysis where it immediately became clear is that one neuron connects with another neuron by putting many synapses. So, this is not a point contact, it is a hand contact. Lots of synapses onto a single neuron. And this is extremely important because it is a very strong rule in the neocortex and you will see that this rule later is one of the most important rules that allows you to actually predict connectivity in the whole circuit. This is what is called the multi-synapse rule connectivity in the whole circuit. This is what is called the multi-synapse rule and we are going to discuss it a lot more about that in the future. The number of connections varies between the different pairs of cells some of them can have an average of 3, 4, 5, 6, 7, 8 sometimes 10 connections so this is the distribution that you see. The probability that one neuron will connect with another neuron is also quite low, it is around Pc=0.1 or 10% and we will discuss in future lectures what combining this knowledge with this knowledge means for connectivity. It has actually a lot of relevance for understanding how neurons connect with each other. So, this is just a view. You can see that when multiple neurons connect there is a cloud of synapses that are formed so you can imagine when one neuron communicates there is a cloud of synapses that are formed so you can imagine when one neuron communicates with all these others, it is actually triggering lots and lots of little voltages in all the other neurons. So this is not going to reveal how synapses work this you should read up in textbooks but just to remind you, you have the axon coming in it forms a bouton, actually pretty much it doesn't look like this, it has a bouton and the bouton has got mitochondria which you also see here, they are the powerhouses they produce ATP so that this thing can run it is below a micron, it is like half a micron typically and when the action potential propagates down the axon, it gets here, the voltage shocks the release of these vesicles and the chemical, the neuro-transmitter, diffuses to the other side in this case Glutamate in the excitatory connection and it activates receptors, different kinds of receptors here, AMPA, NMDA, Kainite and when these receptors are activated they open and ions flow in. Sodium, potassium may flow out, calcium may flow in and so on. But, let's look a little bit at the synaptic dynamics so as we said, we have one neuron it has in this case six synapses, and if you isolate and this you can do in the model, you isolate each of these that are located at different places, on the (INAUDIBLE) right and you look at this one individually and this one individually you can see that they have very different behaviors. Different amplitudes, different rise times delay times, and what you see in the cell body is also quite different. So, the location is very important and one has to bare in mind that when the single neuron communicates with a cell So, the location is very important and one has to bare in mind that when the single neuron communicates with a cell each of the \"fingers\" that are communicating is actually exerting a different effect on the target cell. Okay, let's briefly look at stochastic synaptic transmission. This is a very big field as well, and there is a lot of very interesting ways to analyze how synapses work and you use methods called quantal analysis, where for example if only one vesicle or one synapse fired you would see this sort of response and that would be called the quantal size. If two fired synapses are released, you would see a double amplitude if three, you would see a triple, and so on. And in systems where these are very nice and clearly separated you see these bumps. But in the central nervous system, in the neocortex because the synapses are in different places in the dendrites this becomes blurred and you basically see a very blurry distribution. So it is very difficult to do what is called quantal analysis in central nervous system neurons and it is important if you can do it because you can actually identify how many vesicles, what is the amplitude what we call the quantal size how much impact does a single vesicle have or a single synapse have on the post-synaptic side. As an exercise, try to answer for yourself the following question. What is the probability of successful transmission if you have five synapses in a connection, and each with a probability of 0.5 of releasing when an action potential comes along? So, you got five synapses, each one has the probability of 50% chance of releasing. What is the chance that you will have a complete failure? Or what is the chance that it will always be a success? Also calculate what is the mean amplitude if each quantal size was just one. So what would be the average five, each one with the 0.5 probability, they each produce one if they worked what would be the average? And calculate what is the coefficient of variation of the transmission. If you need help, look at this paper. Okay, let's look at some caveats. So, the first caveats is that it is very difficult to obtain detailed characterizations of synaptic connections. What I showed you is one connection that is being extensively characterized and since then we have only partially characterized up to about twenty-two other connections. And there are hundreds that remain and that is one of the reason why I have to do reconstruction because you can actually predict most of those others now and that is what we are going to do during the course of this MOOC and illustrate how that is done. You also cannot verify easily that when an axon touches a dendrite that it is a real synapse and you can't always do electron microscopy. So, you do have to have data that tells you what is your confidence that this is a synapse. And then the last caveat is that the classical quantal analysis where you can kind of peel out what is the effective and individual synaptic vesicle or an individua synapse and the probability that it is releasing the size that it is releasing and how the whole connection works is quite difficult because of the uncertainty about the locations, the locations are distributed the number of vesicles that are released maybe one or two or three and then you may have many synapses. So, these things make the quantal analysis more difficult in central nervous system but it is not impossible there are statistical approaches around it. And to summarize what we learned in this lecture, the most important thing to remember is all synaptic connections in the neocortex are multi-synapse connections they are hand connections, they are not finger connections. Synaptic responses vary because every release site is probabilistic, the vesicle can release probabilistically, different synapses can behave probabilistically, the size of the vesicle can also vary slightly not hugely, but slightly and then the locations of the synapses on the dendrites can vary a lot. The third piece of information that you learned here was that the number of the synapses per connections depends on the type of connection, so if I have... I showed you an example, where it is just parametal cells and they may form between 3 and 8 connections if you go to a different type of cell it may be between 5 and 20 synapses and so every pair actually has its own range of numbers of connections. And then lastly, because of stochasticity in both the latency and amplitude vary, with the action potential one has to bear in mind that it is a really very noisy signal that is being transmitted from one neuron to another. Okay, that is it for this lecture. \n ---------- \nIn this lecture we are going to focus on synaptic dynamics. In the previous one, we touched briefly on the quantal size of the kinetics, of the strengths of the synapse the amplitude of the synapse, and now we are going to focus on the dynamics of transmission in particular. So, when you record from four neurons, 1, 2, 3, 4 it is not that they are all connected to each other. You will find that 10% are connected so there is a 10% chance that they are going to be connected to each other in this case it maybe that this one is connected to this one but remember with multiple synapses. Now, what I showed you previously was that it was what was the response to a single action potential that you produce this EPSP. EPSP as your action potential. But if you give a train of action potentials so it is a high frequency train of action potentials you see that the amplitude starts dropping and if you then wait, you wait for 500ms and you give another action potential you see that it is recovered to some extent, it hasn't completely recovered, you would have to wait longer before it recovers to this level. This is called synaptic depression. Now, it is extremely important because every synapse has some synaptic depression. So they are not linear synapses, there isn't any case in the neocortex at least where every spike has the same effect if it is given in a high frequency. So this is synaptic depression, so you have the conductance which is the amplitude, how big is this response and that is only for the first action potential which you see here as well, and then if you give a train you see this drop, so we call this the weights and what most people know in neuroscience and most people use in neuroscience and when they build models of the brain and neural network models they have two neurons that are connected and they adjust the weights. This is what they are talking about as what is the magnitude of the response when one neuron communicates with another neuron but it is not that simple and it is actually much more exciting that there is also an interesting dynamic in the transmission so it is almost as if the synapse gets tired of communicating you start communicating, it is a loud sound in the beginning and then it gets softer. Now, it has interesting behavior interesting properties of synaptic depression is that if you start off with 10Hz, you see this kind of depression and then you go to 20Hz, 30Hz, 40Hz and it depresses faster. If you plotted on a curve, you see that the synaptic amplitude drops significantly and these two curves are actually in high calcium and low calcium. This is the low calcium case. The reason why we drop calcium is because it lowers the probability of release so that the EPSP starts off smaller. If the calcium is high it starts off bigger and depresses faster. But what is interesting is that, at a certain frequency the amplitude drops inversely proportional to the frequency. It is what we call one of F behavior and that is important because it means that beyond this frequency you can fire the presynaptic cell as fast as you want but the product of the amplitude and the frequency stays constant. Which means you are screaming as loud as you want and the louder you scream the more the amplitude is adjusted so that you hear exactly the same thing, and you can see that here the steady state amplitude plateaus. So that means that neurons have a limit in how they are communicating with this you can't just jack up the volume you can't just communicate faster and harder there is a dynamic, it is reacting to how you communicate and this is going to turn out to be extremely important shaping the dynamics of neural circuits and we believe they are fundamental to the emergence of higher level cognitive functions. Now, an interesting principal, there are a few principals we have learned from this and the first is that, when you... these are three cells here there are three cells here, one, two, three and one cell is communicating to both these cells so here is the action potential train from cell one that is communicating to the other three and in one cell you can see a strong response and in other cell you see a small response and that is probably just because it has less synapses so it is just connecting with maybe two or three or four synapses and here it is maybe connecting with five or six synapses but also because the probability of release here may be low and the probability of release here may be high. The principal here is that even though they have different amplitudes they all show depression, and so the principal is, when these types of cells communicate with each other these are layer five parametal cells, they depress. And this is going to be important as you are will see in the next few slides. Now, what you also find is that if you have two of the same type of cell and they communicate with each other you see the depression, which is what you see here. But, if it is also the same axon and it touches another type of cell suddenly you see an opposite type of behavior the synapse gets stronger and actually it even gets so stronger it starts firing this other cell. So, this singe cell can start communicating and then it kind of warms up and it gets stronger until this cell reacts. This is called synaptic facilitation. Okay. Synaptic facilitation has another interesting dynamic in response to frequency. So, when you increase the frequency here the amplitude first grows, and then it reaches a peak and then it starts depressing, and it starts depressing because this synapse also has depression it is just that it has a slower time constant for the depression. So, first you see the facilitation, that you see here that makes the amplitude grow and then it drops and what is interesting is that depending on how much facilitation there is you can actually calculate when it is going to reach its peak frequency. This is very important because that means that these synapses behave like bandpass filters. At a certain frequency they are going to be the strongest. If the frequency is lower they are going to be very weak and if it is much higher, they are going to be very weak as well. So, there is a certain frequency that that they have got to fire in order to have a maximum effect on the other cell. What this translates into is that if you have poisson type of firing which is what you have when you record from neurons in the brain actually it doesn't look like this, it needs structure it looks like there is noise, they are just firing and if you apply that kind of firing to a pre-synaptic cell and you look at the response you see wildly fluctuating amplitudes so you do not get a constant amplitude which is what most people today still even ten years after this discovery, most people still use in neural networks constant amplitude synapses with just the weight and not with the dynamics. But this is how all synapses in the neocortex behave. Now there are three classes there are the synapses that facilitate, they grow, and we classify them and you can actually distinguish them statistically you can cluster them and you can show that these are different classes these will occur between certain types of cells and not between other types of cells and then you have the depressing synapses and then you have what we call pseudo linear synapses, synapses that with a facilitation and depression are almost balanced so the synapse can fire at very high frequencies and they only show a mild depression. Now what is interesting is that if you look at a single neuron now you have to consider that there are many different types of cells that are communicating with this cell and each of them can use a different type of synapse. That means that this neuron is interpreting even exactly the same action potential train that may be coming even if they could inject the same action potential train from each of these cells this neuron would interpret this message differently from each neuron, for each neuron. But it doesn't just stop there, it is because these synapses are also distributed in many different places. So, now you can see that what a single neuron listens to, when it is listening to this crowd of other neurons around it depending on which cell it is listening to, those synapses are sitting on different parts of the dendrite and it is interpreting the input from those different parts of the dendrite in a unique way specific to that cell. So it is almost as if every neuron has its own way of selectively listening to specific types of cells. And this complicated map actually is just a map of what kind of synapse you are going to place or exist between different types of cells. There are actually thousands, as close to three thousand different types of connections in this micro circuitry of the neocortex and we haven't recorded from all of them but what we have discovered was a set of principles that allow us to extrapolate that if you have a parametal cell and it is connecting with the Martinotti cell or any that have these electrical properties then it is going to use this type of synapse and if it is connecting onto a basket cell that has these electrical properties then it is going to use this type of synapse. This way we actually have a way today to almost completely specify what kind of synapse exist between different pairs of cells even though we haven't recorded from all of them. So, the caveats in what I have showed you today is that synaptic dynamics may change during recording which I haven't told you but it is because the dynamics in a synapse may require an energy supply and if you patch a cell you may dilute the energy supply ATP, and creating phosphate and other support systems. So one has to be careful when studying dynamics to ensure that you are doing it in a way that you are not depleting to see a sort of rundown of the dynamics. The second caveat is that there is a combinatorial principal, it depends on the identity of the pre and the post synaptic cell as to what kind of synapse they are going to put in there. That means that you really need to solve a very big puzzle where you have all your cell types, versus all your cell types and you have to find out what type of synapse to put between all of them and that is not possible to do, even for a small piece of the brain let alone the whole brain but what is possible is to get some principal so you can infer that between this type of cell and that type of cell they are going to place this type of synapse. The third caveat is that you need to understand dynamics you have to stimulate the pre-synaptic cell with a train of action potentials. Many people stimulate with one spike then you are blind to the dynamics and if you stimulate with two spikes you can look into the theory it is impossible to work out what the dynamics are. So, what is called the pen pulse protocol is insufficient to characterize synaptic dynamics and it is an important caveat to remember if one wants to characterize or interpret synaptic data that one finds in the literature. Okay, so what you learned in this lecture was that there are strong frequency dependence synaptic dynamics in all connections in the neocortex there is no exceptions. Most excitatory to excitatory synaptic transmission shows synaptic depression although there is depression between all kinds of cells and in some you have synaptic facilitation, typically onto certain types of inter-neurons or inhibitory cells, so they get activated with a delay. And then the same axon can travel and have a depressing synapse and then the next bouton can have a facilitating synapse so when a single neuron communicates with a thousand other neurons it actually has different synapses to interpret the same message. So it is like I am sending a broadcast signal, and everybody that is listening hears something different. So, the last thing you have learned is that the synaptic dynamics depends on both the pre and post synaptic neurons so it is a partnering, so you take any two people, it depends on the identities of the two people as to how they are going to interpret what each other are saying. Same person talks to another person there is a different interpretation because there is a different synapse. Okay, that is it for this time, thank you. \n ---------- \n"
}